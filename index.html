
<!DOCTYPE html>

<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Identifier des gènes biosynthétiques avec l’apprentissage par renforcement &#8212; Documentation Trouver des BGC avec l&#39;apprentissage par renforcement 1.0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="_static/translations.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="fr">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint single-page" id="site-navigation">
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/index.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Identifier des gènes biosynthétiques avec l’apprentissage par renforcement
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mise-en-contexte">
     Mise en contexte
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#survol-de-l-apprentissage-par-renforcement">
     Survol de l’apprentissage par renforcement
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#l-algorithme-q-learning">
       L’algorithme Q-learning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exemple-d-utilisation-du-q-learning">
       Exemple d’utilisation du Q-learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-apprentissage-par-renforcement-pour-identifier-des-bcg">
     L’apprentissage par renforcement pour identifier des BCG
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#presentation-de-la-problematique">
       Présentation de la problématique
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ensembles-de-donnees">
       Ensembles de données
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#identification-des-bcg">
       Identification des BCG
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#autres-approches-possibles">
       Autres approches possibles
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliographie">
   Bibliographie
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Identifier des gènes biosynthétiques avec l’apprentissage par renforcement</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Identifier des gènes biosynthétiques avec l’apprentissage par renforcement
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mise-en-contexte">
     Mise en contexte
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#survol-de-l-apprentissage-par-renforcement">
     Survol de l’apprentissage par renforcement
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#l-algorithme-q-learning">
       L’algorithme Q-learning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exemple-d-utilisation-du-q-learning">
       Exemple d’utilisation du Q-learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-apprentissage-par-renforcement-pour-identifier-des-bcg">
     L’apprentissage par renforcement pour identifier des BCG
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#presentation-de-la-problematique">
       Présentation de la problématique
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ensembles-de-donnees">
       Ensembles de données
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#identification-des-bcg">
       Identification des BCG
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#autres-approches-possibles">
       Autres approches possibles
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliographie">
   Bibliographie
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="identifier-des-genes-biosynthetiques-avec-l-apprentissage-par-renforcement">
<h1>Identifier des gènes biosynthétiques avec l’apprentissage par renforcement<a class="headerlink" href="#identifier-des-genes-biosynthetiques-avec-l-apprentissage-par-renforcement" title="Lien permanent vers ce titre">#</a></h1>
<span class="target" id="id1"></span><p>On découvre avec le génome de plusieurs organismes des composés chimiques
utiles pour des activités humaines. Parmi ceux-ci, les métabolites secondaires
présents, entre autres, chez les plantes et les champignons, ont des
applications variées allant de la médecine à l’agriculture <span id="id6">[<a class="reference internal" href="#id15" title="Min Jin Kwon and others. Beyond the biosynthetic gene cluster paradigm: genome-wide coexpression networks connect clustered and unclustered transcription factors to secondary metabolic pathways. Microbiology spectrum, 2021. doi:10.1128/Spectrum.00898-21.">1</a>]</span>.
Des équipes de recherche s’efforcent de trouver des manières d’identifier
davantage de tels composés en explorant différentes approches. Cette page
présente un survol de techniques utilisées dans ce but.</p>
<div class="contents local topic" id="contenu">
<p class="topic-title">Contenu</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mise-en-contexte" id="id20">Mise en contexte</a></p></li>
<li><p><a class="reference internal" href="#survol-de-l-apprentissage-par-renforcement" id="id21">Survol de l’apprentissage par renforcement</a></p>
<ul>
<li><p><a class="reference internal" href="#l-algorithme-q-learning" id="id22">L’algorithme Q-learning</a></p></li>
<li><p><a class="reference internal" href="#exemple-d-utilisation-du-q-learning" id="id23">Exemple d’utilisation du Q-learning</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#l-apprentissage-par-renforcement-pour-identifier-des-bcg" id="id24">L’apprentissage par renforcement pour identifier des BCG</a></p>
<ul>
<li><p><a class="reference internal" href="#presentation-de-la-problematique" id="id25">Présentation de la problématique</a></p></li>
<li><p><a class="reference internal" href="#ensembles-de-donnees" id="id26">Ensembles de données</a></p></li>
<li><p><a class="reference internal" href="#identification-des-bcg" id="id27">Identification des BCG</a></p></li>
<li><p><a class="reference internal" href="#autres-approches-possibles" id="id28">Autres approches possibles</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="mise-en-contexte">
<h2>Mise en contexte<a class="headerlink" href="#mise-en-contexte" title="Lien permanent vers ce titre">#</a></h2>
<p>Les métabolites secondaires sont des molécules qui n’appartiennent pas au
métabolisme primaire <span id="id7">[<a class="reference internal" href="#id15" title="Min Jin Kwon and others. Beyond the biosynthetic gene cluster paradigm: genome-wide coexpression networks connect clustered and unclustered transcription factors to secondary metabolic pathways. Microbiology spectrum, 2021. doi:10.1128/Spectrum.00898-21.">1</a>]</span>. En plus d’être présents chez une variété
d’organismes, comme les plantes, les champignons ou les bactéries, les
métabolites secondaires remplissent des rôles très diversifiés. Ils peuvent
entre autres, défendre un organisme, lui permettre de communiquer avec ses
semblables et attirer des espèces favorables à sa survie.</p>
<p>Dans l’industrie pharmaceutique et en agriculture, les métabolites secondaires
ont une grande importance puisqu’ils permettent de découvrir des molécules
bénéfiques dans plusieurs domaines, comme la fabrication de médicaments
<span id="id8">[<a class="reference internal" href="#id15" title="Min Jin Kwon and others. Beyond the biosynthetic gene cluster paradigm: genome-wide coexpression networks connect clustered and unclustered transcription factors to secondary metabolic pathways. Microbiology spectrum, 2021. doi:10.1128/Spectrum.00898-21.">1</a>]</span>. Malheureusement, les métabolites secondaires sont difficiles
à identifier. Les voies métaboliques impliquées dans leur synthèse sont
encodées dans le génome de l’organisme sur des grappes (<em>clusters</em>) de gènes
contigus nommés <strong>grappe de gènes biosynthétiques</strong> (<em>Biosynthetic Gene Clusters</em>,
BGC). Étant donné leur grande diversité, les méthodes modernes ont du mal à
identifier efficacement les BCG <span id="id9">[<a class="reference internal" href="#id14" title="Hayda Almeida, Adrian Tsang, and Abdoulaye Baniré Diallo. Improving candidate Biosynthetic Gene Clusters in fungi through reinforcement learning. Bioinformatics, 38(16):3984-3991, 06 2022. URL: https://doi.org/10.1093/bioinformatics/btac420, arXiv:https://academic.oup.com/bioinformatics/article-pdf/38/16/3984/45300943/btac420\_supplementary\_data.pdf, doi:10.1093/bioinformatics/btac420.">2</a>]</span>.</p>
<p>Almeida <em>et al.</em> <span id="id10">[<a class="reference internal" href="#id14" title="Hayda Almeida, Adrian Tsang, and Abdoulaye Baniré Diallo. Improving candidate Biosynthetic Gene Clusters in fungi through reinforcement learning. Bioinformatics, 38(16):3984-3991, 06 2022. URL: https://doi.org/10.1093/bioinformatics/btac420, arXiv:https://academic.oup.com/bioinformatics/article-pdf/38/16/3984/45300943/btac420\_supplementary\_data.pdf, doi:10.1093/bioinformatics/btac420.">2</a>]</span> présentent une technique de détection des
BCG basée sur l’apprentissage par renforcement, un type d’intelligence
artificielle. Cette page présente le fonctionnement de leur méthode ainsi que
des comparaisons avec d’autres approches.</p>
</section>
<section id="survol-de-l-apprentissage-par-renforcement">
<h2>Survol de l’apprentissage par renforcement<a class="headerlink" href="#survol-de-l-apprentissage-par-renforcement" title="Lien permanent vers ce titre">#</a></h2>
<p>L’apprentissage par renforcement consiste à entraîner un modèle en simulant des
interaction avec un environnement. En expérimentant différentes actions dans un
milieu virtuel, un <em>agent</em> apprend quelles <em>actions</em> poser pour atteindre un
<em>état</em> voulu <span id="id11">[<a class="reference internal" href="#id16" title="Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep q-learning algorithms. Proceedings of the 36th International Conference on Machine Learning, 97:2021–2030, 09–15 Jun 2019. URL: https://proceedings.mlr.press/v97/fu19a.html.">3</a>]</span>. Les <em>récompenses</em> sont des valeurs qui
guident l’agent dans son entraînement quand il se rapproche de l’état voulu.</p>
<p>Par exemple, si l’on utilise l’apprentissage par renforcement pour apprendre à
un robot comment marcher en ligne droite, on peut considérer que :</p>
<ul class="simple">
<li><p><strong>L’agent</strong> est le robot.</p></li>
<li><p><strong>L’environnement</strong> est le milieu dans lequel il se déplace.</p></li>
<li><p>Les <strong>états</strong> représentent toutes les configurations possibles de robot et sa
position dans l’espace.</p></li>
<li><p>Les <strong>actions</strong> de l’agent consistent à bouger ses articulations.</p></li>
<li><p>Les <strong>récompenses</strong> mesurent comment l’agent accomplit sa tâches. Par
exemple, avancer pourrait entraîner une récompense positive tandis que tomber
ou reculer entraînerait une récompense négative.</p></li>
</ul>
<section id="l-algorithme-q-learning">
<h3>L’algorithme Q-learning<a class="headerlink" href="#l-algorithme-q-learning" title="Lien permanent vers ce titre">#</a></h3>
<p>L’algorithme Q-learning est une manière d’entraîner un agent. Soient :</p>
<ul class="simple">
<li><p>Un ensemble d’états <span class="math notranslate nohighlight">\(S\)</span> composé de <span class="math notranslate nohighlight">\(n\)</span> états <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>Un ensemble d’actions <span class="math notranslate nohighlight">\(A\)</span> composé de <span class="math notranslate nohighlight">\(m\)</span> actions <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
</ul>
<p>Un agent dans un état <span class="math notranslate nohighlight">\(s_t\)</span> peut poser une action <span class="math notranslate nohighlight">\(a_t\)</span> pour passer
à un état <span class="math notranslate nohighlight">\(s_{t+1}\)</span>. À chaque transition d’état, l’agent observe une
récompense <span class="math notranslate nohighlight">\(r_t\)</span>.</p>
<p>Le but du Q-learning est d’apprendre quelle est la meilleure action à
sélectionner lorsque l’agent est dans un état donné. Pour ce faire, on cherche
à élaborer une <strong>fonction de qualité</strong> <span class="math notranslate nohighlight">\(Q\)</span> qui calcule la qualité
(c’est-à-dire, la tendance à produire des récompenses positives) de chaque
combinaison état-action. L’équation <a class="reference internal" href="#equation-qlearning">(1)</a> présente l’algorithme
utilisé pour calculer la fonction de qualité :</p>
<div class="math notranslate nohighlight" id="equation-qlearning">
<span class="eqno">(1)<a class="headerlink" href="#equation-qlearning" title="Lien permanent vers cette équation">#</a></span>\[Q_{t+1}(s_t, a_t) \leftarrow Q_t(s_t, a_t) + \alpha \{ r_t + \gamma \cdot argmax[Q(s_{t+1}, a)] - Q(s_t, a_t) \}\]</div>
<p>où :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> est le facteur d’apprentissage. Plus
<span class="math notranslate nohighlight">\(\alpha\)</span> est élevé, plus l’algorithme privilégie les informations
récentes par rapport aux informations anciennes. On doit observer
<span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> est le facteur d’actualisation. Plus <span class="math notranslate nohighlight">\(\gamma\)</span> est élevé,
plus l’algorithme privilégie les récompenses à long terme par rapport aux
récompenses à court terme. On doit observer
<span class="math notranslate nohighlight">\(0 &lt; \gamma &lt; 1\)</span>.</p></li>
</ul>
</section>
<section id="exemple-d-utilisation-du-q-learning">
<h3>Exemple d’utilisation du Q-learning<a class="headerlink" href="#exemple-d-utilisation-du-q-learning" title="Lien permanent vers ce titre">#</a></h3>
<p>Cette section présente une application du Q-learning pour résoudre un problème
simple disponible dans la bibliothèque <code class="docutils literal notranslate"><span class="pre">Gymnasium</span></code>, un projet destiné à
étudier l’apprentissage par renforcement <span id="id12">[<a class="reference internal" href="#id17" title="Farama Foundation. Gymnasium documentation. 2023. URL: https://gymnasium.farama.org/ (visited on 2023-02-05).">4</a>]</span>.</p>
<p>L’environnement <code class="docutils literal notranslate"><span class="pre">FrozenLake-v1</span></code> de la bibliothèque permet de tester des
algorithmes. Il est composé d’une grille de seize cellules. L’agent doit se
déplacer d’une position de départ vers une cible en évitant des obstacles.
Atteindre la cible entraîne une récompense de <code class="docutils literal notranslate"><span class="pre">1,0</span></code>.</p>
<p>Au début de l’entraînement, l’agent ne sait pas quelles actions sélectionner
pour atteindre l’objectif. Il pose des actions aléatoires qui ne lui permettent
pas d’obtenir des valeurs de récompense, comme le montre l’animation suivante :</p>
<figure class="align-default" id="id18">
<img alt="_images/frozen_lake_aleatoire.gif" src="_images/frozen_lake_aleatoire.gif" />
<figcaption>
<p><span class="caption-text">Exploration aléatoire de l’environnement</span><a class="headerlink" href="#id18" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<p>Pour améliorer les performances, on calcule la fonction de qualité en suivant
l’algorithme Q-learning. Le code Python suivant montre comment appliquer la
fonction <a class="reference internal" href="#equation-qlearning">(1)</a> en interagissant avec l’environnement.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Facteur d&#39;apprentissage (vitesse de changement de la valeur Q)</span>
<span class="linenos"> 2</span><span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="c1"># Facteur d&#39;actualisation (importance des récompenses futures)</span>
<span class="linenos"> 3</span><span class="n">N_EPISODES</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># Nombre d&#39;essais</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="c1"># Créer l&#39;environnement d&#39;apprentissage et la table de qualité.</span>
<span class="linenos"> 6</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v1&quot;</span><span class="p">,</span> <span class="n">is_slippery</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos"> 7</span><span class="n">qtable</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1"># Entraîner le modèle en réinitialisant l&#39;environnement à chaque épisode.</span>
<span class="linenos">10</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_EPISODES</span><span class="p">):</span>
<span class="linenos">11</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">12</span>    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="linenos">13</span>        <span class="c1"># Sélectionner l&#39;action avec la meilleure qualité. Si aucune action</span>
<span class="linenos">14</span>        <span class="c1"># n&#39;a été évaluée pour l&#39;état, choisir une action aléatoirement.</span>
<span class="linenos">15</span>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">16</span>            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
<span class="linenos">17</span>        <span class="k">else</span><span class="p">:</span>
<span class="linenos">18</span>            <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="linenos">19</span>        <span class="c1"># Interagir avec l&#39;environnement et mesurer la réponse.</span>
<span class="linenos">20</span>        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="linenos">21</span>        <span class="k">if</span> <span class="n">truncated</span> <span class="ow">or</span> <span class="n">terminated</span><span class="p">:</span>
<span class="linenos">22</span>            <span class="k">break</span>
<span class="linenos">23</span>        <span class="c1"># Actualiser la table.</span>
<span class="linenos">24</span>        <span class="n">q_0</span> <span class="o">=</span> <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
<span class="linenos">25</span>        <span class="n">q_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">new_state</span><span class="p">])</span>
<span class="linenos">26</span>        <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">GAMMA</span><span class="o">*</span><span class="n">q_1</span> <span class="o">-</span> <span class="n">q_0</span><span class="p">)</span>
<span class="linenos">27</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
<span class="linenos">28</span><span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p>On obtient la table de qualité suivante :</p>
<table class="table">
<colgroup>
<col style="width: 16%" />
<col style="width: 15%" />
<col style="width: 18%" />
<col style="width: 18%" />
<col style="width: 18%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Table de qualité</p></th>
<th class="head"></th>
<th class="head" colspan="4"><p>Action (direction vers laquelle se déplacer)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td></td>
<td><p><span class="math notranslate nohighlight">\(\leftarrow\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\downarrow\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\rightarrow\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\uparrow\)</span></p></td>
</tr>
<tr class="row-odd"><td rowspan="5"><p><strong>État (position
de l’agent)</strong></p></td>
<td><p><span class="math notranslate nohighlight">\((0, 0)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,59\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\((0, 1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(...\)</span></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\((3, 2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\((3, 3)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0,0\)</span></p></td>
</tr>
</tbody>
</table>
<p>Par exemple, on voit que l’algorithme a déterminé que la meilleure action à
sélectionner lorsque l’agent se trouve dans la cellule <span class="math notranslate nohighlight">\((0, 0)\)</span> consiste
se diriger vers le bas (<span class="math notranslate nohighlight">\(\downarrow\)</span>). En suivant ce modèle, l’agent
peut alors se déplacer dans l’environnement sans rencontrer d’obstacle pour
atteindre son but.</p>
<figure class="align-default" id="id19">
<img alt="_images/frozen_lake_qlearning.gif" src="_images/frozen_lake_qlearning.gif" />
<figcaption>
<p><span class="caption-text">Déplacement dans l’environnement après apprentissage par Q-learning</span><a class="headerlink" href="#id19" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="l-apprentissage-par-renforcement-pour-identifier-des-bcg">
<h2>L’apprentissage par renforcement pour identifier des BCG<a class="headerlink" href="#l-apprentissage-par-renforcement-pour-identifier-des-bcg" title="Lien permanent vers ce titre">#</a></h2>
<p>La section suivante présente comment appliquer l’algorithme Q-learning à la
découverte de BCG chez les champignons.</p>
<section id="presentation-de-la-problematique">
<h3>Présentation de la problématique<a class="headerlink" href="#presentation-de-la-problematique" title="Lien permanent vers ce titre">#</a></h3>
</section>
<section id="ensembles-de-donnees">
<h3>Ensembles de données<a class="headerlink" href="#ensembles-de-donnees" title="Lien permanent vers ce titre">#</a></h3>
</section>
<section id="identification-des-bcg">
<h3>Identification des BCG<a class="headerlink" href="#identification-des-bcg" title="Lien permanent vers ce titre">#</a></h3>
</section>
<section id="autres-approches-possibles">
<h3>Autres approches possibles<a class="headerlink" href="#autres-approches-possibles" title="Lien permanent vers ce titre">#</a></h3>
</section>
</section>
</section>
<section id="bibliographie">
<h1>Bibliographie<a class="headerlink" href="#bibliographie" title="Lien permanent vers ce titre">#</a></h1>
<div class="docutils container" id="id13">
<dl class="citation">
<dt class="label" id="id15"><span class="brackets">1</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id7">2</a>,<a href="#id8">3</a>)</span></dt>
<dd><p>Min Jin Kwon and others. Beyond the biosynthetic gene cluster paradigm: genome-wide coexpression networks connect clustered and unclustered transcription factors to secondary metabolic pathways. <em>Microbiology spectrum</em>, 2021. <a class="reference external" href="https://doi.org/10.1128/Spectrum.00898-21">doi:10.1128/Spectrum.00898-21</a>.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">2</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Hayda Almeida, Adrian Tsang, and Abdoulaye Baniré Diallo. Improving candidate Biosynthetic Gene Clusters in fungi through reinforcement learning. <em>Bioinformatics</em>, 38(16):3984–3991, 06 2022. URL: <a class="reference external" href="https://doi.org/10.1093/bioinformatics/btac420">https://doi.org/10.1093/bioinformatics/btac420</a>, <a class="reference external" href="https://arxiv.org/abs/https://academic.oup.com/bioinformatics/article-pdf/38/16/3984/45300943/btac420\_supplementary\_data.pdf">arXiv:https://academic.oup.com/bioinformatics/article-pdf/38/16/3984/45300943/btac420\_supplementary\_data.pdf</a>, <a class="reference external" href="https://doi.org/10.1093/bioinformatics/btac420">doi:10.1093/bioinformatics/btac420</a>.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id11">3</a></span></dt>
<dd><p>Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep q-learning algorithms. <em>Proceedings of the 36th International Conference on Machine Learning</em>, 97:2021–2030, 09–15 Jun 2019. URL: <a class="reference external" href="https://proceedings.mlr.press/v97/fu19a.html">https://proceedings.mlr.press/v97/fu19a.html</a>.</p>
</dd>
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id12">4</a></span></dt>
<dd><p>Farama Foundation. Gymnasium documentation. 2023. URL: <a class="reference external" href="https://gymnasium.farama.org/">https://gymnasium.farama.org/</a> (visited on 2023-02-05).</p>
</dd>
</dl>
</div>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sofia FASSI FEHRI, Vincent THERRIEN<br/>
  
      &copy; Copyright 2023, Sofia FASSI FEHRI, Vincent THERRIEN.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>