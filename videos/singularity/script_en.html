<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>AI Singularity Is A Smokescreen</title>
		<style>@media (prefers-color-scheme:dark){.markdown-body,[data-theme=dark]{color-scheme:dark;--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-brackethighlighter-unmatched:#f85149;--color-prettylights-syntax-invalid-illegal-text:#f0f6fc;--color-prettylights-syntax-invalid-illegal-bg:#8e1519;--color-prettylights-syntax-carriage-return-text:#f0f6fc;--color-prettylights-syntax-carriage-return-bg:#b62324;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-markup-list:#f2cc60;--color-prettylights-syntax-markup-heading:#1f6feb;--color-prettylights-syntax-markup-italic:#c9d1d9;--color-prettylights-syntax-markup-bold:#c9d1d9;--color-prettylights-syntax-markup-deleted-text:#ffdcd7;--color-prettylights-syntax-markup-deleted-bg:#67060c;--color-prettylights-syntax-markup-inserted-text:#aff5b4;--color-prettylights-syntax-markup-inserted-bg:#033a16;--color-prettylights-syntax-markup-changed-text:#ffdfb6;--color-prettylights-syntax-markup-changed-bg:#5a1e02;--color-prettylights-syntax-markup-ignored-text:#c9d1d9;--color-prettylights-syntax-markup-ignored-bg:#1158c7;--color-prettylights-syntax-meta-diff-range:#d2a8ff;--color-prettylights-syntax-brackethighlighter-angle:#8b949e;--color-prettylights-syntax-sublimelinter-gutter-mark:#484f58;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#e6edf3;--color-fg-muted:#848d97;--color-fg-subtle:#6e7681;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,0.4);--color-accent-fg:#2f81f7;--color-accent-emphasis:#1f6feb;--color-success-fg:#3fb950;--color-success-emphasis:#238636;--color-attention-fg:#d29922;--color-attention-emphasis:#9e6a03;--color-attention-subtle:rgba(187,128,9,0.15);--color-danger-fg:#f85149;--color-danger-emphasis:#da3633;--color-done-fg:#a371f7;--color-done-emphasis:#8957e5}}@media (prefers-color-scheme:light){.markdown-body,[data-theme=light]{color-scheme:light;--color-prettylights-syntax-comment:#57606a;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#6639ba;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-brackethighlighter-unmatched:#82071e;--color-prettylights-syntax-invalid-illegal-text:#f6f8fa;--color-prettylights-syntax-invalid-illegal-bg:#82071e;--color-prettylights-syntax-carriage-return-text:#f6f8fa;--color-prettylights-syntax-carriage-return-bg:#cf222e;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-markup-list:#3b2300;--color-prettylights-syntax-markup-heading:#0550ae;--color-prettylights-syntax-markup-italic:#24292f;--color-prettylights-syntax-markup-bold:#24292f;--color-prettylights-syntax-markup-deleted-text:#82071e;--color-prettylights-syntax-markup-deleted-bg:#ffebe9;--color-prettylights-syntax-markup-inserted-text:#116329;--color-prettylights-syntax-markup-inserted-bg:#dafbe1;--color-prettylights-syntax-markup-changed-text:#953800;--color-prettylights-syntax-markup-changed-bg:#ffd8b5;--color-prettylights-syntax-markup-ignored-text:#eaeef2;--color-prettylights-syntax-markup-ignored-bg:#0550ae;--color-prettylights-syntax-meta-diff-range:#8250df;--color-prettylights-syntax-brackethighlighter-angle:#57606a;--color-prettylights-syntax-sublimelinter-gutter-mark:#8c959f;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#1F2328;--color-fg-muted:#656d76;--color-fg-subtle:#6e7781;--color-canvas-default:#ffffff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:hsla(210,18%,87%,1);--color-neutral-muted:rgba(175,184,193,0.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-success-fg:#1a7f37;--color-success-emphasis:#1f883d;--color-attention-fg:#9a6700;--color-attention-emphasis:#9a6700;--color-attention-subtle:#fff8c5;--color-danger-fg:#d1242f;--color-danger-emphasis:#cf222e;--color-done-fg:#8250df;--color-done-emphasis:#8250df}}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;margin:0;color:var(--color-fg-default);background-color:var(--color-canvas-default);font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","Noto Sans",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji";font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body h1:hover .anchor .octicon-link:before,.markdown-body h2:hover .anchor .octicon-link:before,.markdown-body h3:hover .anchor .octicon-link:before,.markdown-body h4:hover .anchor .octicon-link:before,.markdown-body h5:hover .anchor .octicon-link:before,.markdown-body h6:hover .anchor .octicon-link:before{width:16px;height:16px;content:' ';display:inline-block;background-color:currentColor;-webkit-mask-image:url("data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' version='1.1' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg>");mask-image:url("data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' version='1.1' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg>")}.markdown-body details,.markdown-body figcaption,.markdown-body figure{display:block}.markdown-body summary{display:list-item}.markdown-body [hidden]{display:none!important}.markdown-body a{background-color:transparent;color:var(--color-accent-fg);text-decoration:none}.markdown-body abbr[title]{border-bottom:none;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.markdown-body b,.markdown-body strong{font-weight:var(--base-text-weight-semibold,600)}.markdown-body dfn{font-style:italic}.markdown-body h1{margin:.67em 0;font-weight:var(--base-text-weight-semibold,600);padding-bottom:.3em;font-size:2em;border-bottom:1px solid var(--color-border-muted)}.markdown-body mark{background-color:var(--color-attention-subtle);color:var(--color-fg-default)}.markdown-body small{font-size:90%}.markdown-body sub,.markdown-body sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}.markdown-body sub{bottom:-.25em}.markdown-body sup{top:-.5em}.markdown-body img{border-style:none;max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body code,.markdown-body kbd,.markdown-body pre,.markdown-body samp{font-family:monospace;font-size:1em}.markdown-body figure{margin:1em 40px}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid var(--color-border-muted);height:.25em;padding:0;margin:24px 0;background-color:var(--color-border-default);border:0}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=button],.markdown-body [type=reset],.markdown-body [type=submit]{-webkit-appearance:button;appearance:button}.markdown-body [type=checkbox],.markdown-body [type=radio]{box-sizing:border-box;padding:0}.markdown-body [type=number]::-webkit-inner-spin-button,.markdown-body [type=number]::-webkit-outer-spin-button{height:auto}.markdown-body [type=search]::-webkit-search-cancel-button,.markdown-body [type=search]::-webkit-search-decoration{-webkit-appearance:none;appearance:none}.markdown-body ::-webkit-input-placeholder{color:inherit;opacity:.54}.markdown-body ::-webkit-file-upload-button{-webkit-appearance:button;appearance:button;font:inherit}.markdown-body a:hover{text-decoration:underline}.markdown-body ::placeholder{color:var(--color-fg-subtle);opacity:1}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:max-content;max-width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none!important}.markdown-body [role=button]:focus,.markdown-body a:focus,.markdown-body input[type=checkbox]:focus,.markdown-body input[type=radio]:focus{outline:2px solid var(--color-accent-fg);outline-offset:-2px;box-shadow:none}.markdown-body [role=button]:focus:not(:focus-visible),.markdown-body a:focus:not(:focus-visible),.markdown-body input[type=checkbox]:focus:not(:focus-visible),.markdown-body input[type=radio]:focus:not(:focus-visible){outline:solid 1px transparent}.markdown-body [role=button]:focus-visible,.markdown-body a:focus-visible,.markdown-body input[type=checkbox]:focus-visible,.markdown-body input[type=radio]:focus-visible{outline:2px solid var(--color-accent-fg);outline-offset:-2px;box-shadow:none}.markdown-body a:not([class]):focus,.markdown-body a:not([class]):focus-visible,.markdown-body input[type=checkbox]:focus,.markdown-body input[type=checkbox]:focus-visible,.markdown-body input[type=radio]:focus,.markdown-body input[type=radio]:focus-visible{outline-offset:0}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px ui-monospace,SFMono-Regular,SF Mono,Menlo,Consolas,Liberation Mono,monospace;line-height:10px;color:var(--color-fg-default);vertical-align:middle;background-color:var(--color-canvas-subtle);border:solid 1px var(--color-neutral-muted);border-bottom-color:var(--color-neutral-muted);border-radius:6px;box-shadow:inset 0 -1px 0 var(--color-neutral-muted)}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:var(--base-text-weight-semibold,600);line-height:1.25}.markdown-body h2{font-weight:var(--base-text-weight-semibold,600);padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid var(--color-border-muted)}.markdown-body h3{font-weight:var(--base-text-weight-semibold,600);font-size:1.25em}.markdown-body h4{font-weight:var(--base-text-weight-semibold,600);font-size:1em}.markdown-body h5{font-weight:var(--base-text-weight-semibold,600);font-size:.875em}.markdown-body h6{font-weight:var(--base-text-weight-semibold,600);font-size:.85em;color:var(--color-fg-muted)}.markdown-body p{margin-top:0;margin-bottom:10px}.markdown-body blockquote{margin:0;padding:0 1em;color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default)}.markdown-body ol,.markdown-body ul{margin-top:0;margin-bottom:0;padding-left:2em}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code,.markdown-body samp,.markdown-body tt{font-family:ui-monospace,SFMono-Regular,SF Mono,Menlo,Consolas,Liberation Mono,monospace;font-size:12px}.markdown-body pre{margin-top:0;margin-bottom:0;font-family:ui-monospace,SFMono-Regular,SF Mono,Menlo,Consolas,Liberation Mono,monospace;font-size:12px;word-wrap:normal}.markdown-body .octicon{display:inline-block;overflow:visible!important;vertical-align:text-bottom;fill:currentColor}.markdown-body input::-webkit-inner-spin-button,.markdown-body input::-webkit-outer-spin-button{margin:0;-webkit-appearance:none;appearance:none}.markdown-body .mr-2{margin-right:var(--base-size-8,8px)!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body details,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{padding:0 .2em;font-size:inherit}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{padding-bottom:0;border-bottom:0}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol[type="a s"]{list-style-type:lower-alpha}.markdown-body ol[type="A s"]{list-style-type:upper-alpha}.markdown-body ol[type="i s"]{list-style-type:lower-roman}.markdown-body ol[type="I s"]{list-style-type:upper-roman}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:var(--base-text-weight-semibold,600)}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:var(--base-text-weight-semibold,600)}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid var(--color-border-default)}.markdown-body table td>:last-child{margin-bottom:0}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:transparent}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid var(--color-border-default)}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:var(--color-fg-default)}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:.2em .4em;margin:0;font-size:85%;white-space:break-spaces;background-color:var(--color-neutral-muted);border-radius:6px}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre code{font-size:100%}.markdown-body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;color:var(--color-fg-default);background-color:var(--color-canvas-subtle);border-radius:6px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-num{padding:10px 8px 9px;text-align:right;background:var(--color-canvas-default);border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:var(--base-text-weight-semibold,600);background:var(--color-canvas-subtle);border-top:0}.markdown-body [data-footnote-ref]::before{content:"["}.markdown-body [data-footnote-ref]::after{content:"]"}.markdown-body .footnotes{font-size:12px;color:var(--color-fg-muted);border-top:1px solid var(--color-border-default)}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes ol ul{display:inline-block;padding-left:16px;margin-top:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target::before{position:absolute;top:-8px;right:-8px;bottom:-8px;left:-24px;pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body .pl-c{color:var(--color-prettylights-syntax-comment)}.markdown-body .pl-c1,.markdown-body .pl-s .pl-v{color:var(--color-prettylights-syntax-constant)}.markdown-body .pl-e,.markdown-body .pl-en{color:var(--color-prettylights-syntax-entity)}.markdown-body .pl-s .pl-s1,.markdown-body .pl-smi{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .pl-ent{color:var(--color-prettylights-syntax-entity-tag)}.markdown-body .pl-k{color:var(--color-prettylights-syntax-keyword)}.markdown-body .pl-pds,.markdown-body .pl-s,.markdown-body .pl-s .pl-pse .pl-s1,.markdown-body .pl-sr,.markdown-body .pl-sr .pl-cce,.markdown-body .pl-sr .pl-sra,.markdown-body .pl-sr .pl-sre{color:var(--color-prettylights-syntax-string)}.markdown-body .pl-smw,.markdown-body .pl-v{color:var(--color-prettylights-syntax-variable)}.markdown-body .pl-bu{color:var(--color-prettylights-syntax-brackethighlighter-unmatched)}.markdown-body .pl-ii{color:var(--color-prettylights-syntax-invalid-illegal-text);background-color:var(--color-prettylights-syntax-invalid-illegal-bg)}.markdown-body .pl-c2{color:var(--color-prettylights-syntax-carriage-return-text);background-color:var(--color-prettylights-syntax-carriage-return-bg)}.markdown-body .pl-sr .pl-cce{font-weight:700;color:var(--color-prettylights-syntax-string-regexp)}.markdown-body .pl-ml{color:var(--color-prettylights-syntax-markup-list)}.markdown-body .pl-mh,.markdown-body .pl-mh .pl-en,.markdown-body .pl-ms{font-weight:700;color:var(--color-prettylights-syntax-markup-heading)}.markdown-body .pl-mi{font-style:italic;color:var(--color-prettylights-syntax-markup-italic)}.markdown-body .pl-mb{font-weight:700;color:var(--color-prettylights-syntax-markup-bold)}.markdown-body .pl-md{color:var(--color-prettylights-syntax-markup-deleted-text);background-color:var(--color-prettylights-syntax-markup-deleted-bg)}.markdown-body .pl-mi1{color:var(--color-prettylights-syntax-markup-inserted-text);background-color:var(--color-prettylights-syntax-markup-inserted-bg)}.markdown-body .pl-mc{color:var(--color-prettylights-syntax-markup-changed-text);background-color:var(--color-prettylights-syntax-markup-changed-bg)}.markdown-body .pl-mi2{color:var(--color-prettylights-syntax-markup-ignored-text);background-color:var(--color-prettylights-syntax-markup-ignored-bg)}.markdown-body .pl-mdr{font-weight:700;color:var(--color-prettylights-syntax-meta-diff-range)}.markdown-body .pl-ba{color:var(--color-prettylights-syntax-brackethighlighter-angle)}.markdown-body .pl-sg{color:var(--color-prettylights-syntax-sublimelinter-gutter-mark)}.markdown-body .pl-corl{text-decoration:underline;color:var(--color-prettylights-syntax-constant-other-reference-link)}.markdown-body g-emoji{display:inline-block;min-width:1ch;font-family:"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:1em;font-style:normal!important;font-weight:var(--base-text-weight-normal,400);line-height:1;vertical-align:-.075em}.markdown-body g-emoji img{width:1em;height:1em}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item label{font-weight:var(--base-text-weight-normal,400)}.markdown-body .task-list-item.enabled label{cursor:pointer}.markdown-body .task-list-item+.task-list-item{margin-top:4px}.markdown-body .task-list-item .handle{display:none}.markdown-body .task-list-item-checkbox{margin:0 .2em .25em -1.4em;vertical-align:middle}.markdown-body .contains-task-list:dir(rtl) .task-list-item-checkbox{margin:0 -1.6em .25em .2em}.markdown-body .contains-task-list{position:relative}.markdown-body .contains-task-list:focus-within .task-list-item-convert-container,.markdown-body .contains-task-list:hover .task-list-item-convert-container{display:block;width:auto;height:24px;overflow:visible;clip:auto}.markdown-body ::-webkit-calendar-picker-indicator{filter:invert(50%)}.markdown-body .markdown-alert{padding:var(--base-size-8) var(--base-size-16);margin-bottom:16px;color:inherit;border-left:.25em solid var(--color-border-default)}.markdown-body .markdown-alert>:first-child{margin-top:0}.markdown-body .markdown-alert>:last-child{margin-bottom:0}.markdown-body .markdown-alert .markdown-alert-title{display:flex;font-weight:var(--base-text-weight-medium,500);align-items:center;line-height:1}.markdown-body .markdown-alert.markdown-alert-note{border-left-color:var(--color-accent-emphasis)}.markdown-body .markdown-alert.markdown-alert-note .markdown-alert-title{color:var(--color-accent-fg)}.markdown-body .markdown-alert.markdown-alert-important{border-left-color:var(--color-done-emphasis)}.markdown-body .markdown-alert.markdown-alert-important .markdown-alert-title{color:var(--color-done-fg)}.markdown-body .markdown-alert.markdown-alert-warning{border-left-color:var(--color-attention-emphasis)}.markdown-body .markdown-alert.markdown-alert-warning .markdown-alert-title{color:var(--color-attention-fg)}.markdown-body .markdown-alert.markdown-alert-tip{border-left-color:var(--color-success-emphasis)}.markdown-body .markdown-alert.markdown-alert-tip .markdown-alert-title{color:var(--color-success-fg)}.markdown-body .markdown-alert.markdown-alert-caution{border-left-color:var(--color-danger-emphasis)}.markdown-body .markdown-alert.markdown-alert-caution .markdown-alert-title{color:var(--color-danger-fg)}</style>
		<style>
			body {
				margin: 0;
			}

			.markdown-body-content {
				box-sizing: border-box;
				min-width: 200px;
				max-width: 980px;
				margin: 0 auto;
				padding: 45px;
			}

			@media (max-width: 767px) {
				.markdown-body-content {
					padding: 15px;
				}
			}

			.markdown-body {
				--base-size-8: 8px;
				--base-size-16: 16px;
			}
		</style>
	</head>
	<body class="markdown-body">
		<article class="markdown-body-content"><h1>AI Singularity Is A Smokescreen</h1>
<p>Whenever I curse my eyes with the worse app in existence, I get to read the same few takes about AI.<br>
You probably know what I'm talking about:</p>
<ul>
<li>"AI will replace everyone"</li>
<li>"AI will never replace everyone"</li>
<li>"Buy my course to avoid getting replaced by AI"</li>
<li>And my favorite: "The singularity is almost here"</li>
</ul>
<p>Most people are prone to exaggeration when they talk about the future. It's easy to predict that<br>
cars will be self=driving by 2018 [1]. That Amazon will fully automate grocery stores [2] [3].<br>
That programmers will get replaced [4] [5]. Not that these technologies are useless, they <em>are</em><br>
impressive, but they end up being less transformative than expected. The wildest exaggeration out<br>
there is artificial general intelligence, the idea that an AI could outperform humans in any context<br>
[6]. It's hypothetical at the moment, but tech CEOs say it's almost here [7]. It's good for the<br>
investors I guess. One possible consequence of AGI is the singularity, a point at which machines<br>
can improve themselves so fast that human would not be able to keep up and would get replaced [8].</p>
<p>I think that the singularity is a distraction. Not only because it's exaggerated, but mostly<br>
because it shifts attention away from real issues like poor governance, low sustainability, loss of<br>
critical thinking. They all get exacerbated by AI and we need to address them now instead of<br>
daydreaming about the singularity. And before we start, I will just spell out that I'm not against<br>
AI. It does contribute to science and productivity and other things. I just want to reframe the<br>
discussion around facts instead of hype.</p>
<h2>I. Reasoning</h2>
<p>The first point I'll address is reasoning. It's been talked to death already, but generative AI<br>
models don't think, they replicate statistical patterns, which makes them hallucinate false<br>
information [9]. The counterpoint you hear all the time is "sure, current models are not perfect,<br>
but this is just the start of the hockey stick. AI will ingest more and more data, get exponentially<br>
better, and inevitably reach superintelligence!".</p>
<p>The hockey stick fallacy happens, in my opinion, because of a misunderstanding of emergent<br>
properties, that is, properties that appear when assembling elements that individually lack these<br>
properties. A few researchers have claimed that language models display emergent properties. Small<br>
models are weak, but they improve as you increase their size and, passed, a certain point, their<br>
performance shoots up, as if intelligence had somehow emerged [10]. This is just a superficial<br>
impression that appears when using simplistic metrics. There's no spark of intelligence that appears<br>
during training. If we use better statistics, it's easy to realize that performance gradually<br>
improves and that emergent properties are an illusion [11]. The models remain bound to the same<br>
limitations and will continue to hallucinate, even if we mitigate them by using a bigger training<br>
dataset [12] [13] [55]. <em>Maybe</em> true intelligence will emerge if we keep using more data or better<br>
architectures or improvement feedback loops or whatever, but there's no proof that will happen.</p>
<p>The consensus at the moment is that AI does not reason because it's too different from human<br>
cognition, but some people disagree. AI parrots intelligence, but it does it so well enough to solve<br>
problems, which, one could say, qualifies as reasoning. Either position is valid depending on how<br>
you define "reasoning", that's just semantics. And yet, you can find tons of people online defending<br>
these opinions, but they will never stop debating because we don't have a universal definition of<br>
intelligence. That's arguing for the sake of arguing. I don't find that conversation useful. We<br>
should instead discuss the limitations of AI.</p>
<p>No matter how good language models gets, their hallucinations remain unpredictable. If no one steps<br>
in to eliminate them, they might get amplified [54], which makes AI, at the moment, unsuitable for<br>
critical applications like medicine [14] or law [15]. And even, to some degree, in less critical<br>
applications. The translation market is projected to grow in the next years [16], not because AI<br>
cannot translate, but because humans will always add value to translation. I'm not saying that we<br>
should refrain from using AI or that it won't impact the job market, it can automate a lot of work<br>
pretty well, but we have to remain aware of its limitations and realize that they will not<br>
necessarily go away.</p>
<h2>II. Interpretability</h2>
<p>Another problem with AI is interpretability, understanding how It works. A lot of models, like<br>
neural networks, are so large and convoluted that it's hard for humans to figure out how they<br>
generate data. Other models are hidden behind paywalls, so we cannot inspect them. Those are called<br>
black box models: we know what goes in and what goes out, but what happens in the middle is<br>
incomprehensible [17].</p>
<p>And that can cause social problems. COMPAS, for instance, is a "recidivism risk prediction tool". It<br>
basically tries to guess how likely a criminal is to commit another crime later, and some judges in<br>
the US use it to decide if they send people to jail [53]. COMPAS is a proprietary algorithm, so no<br>
one outside of the company that makes it knows exactly how it works which is not ideal. If you<br>
disagree with a judge's decision and have valid reasons for that, you can appeal and have it<br>
revised. But imagine if the decision is based on a biased model. Or a model that uses incorrect<br>
information. You have no way of proving that the algorithm is at fault because its inner working is<br>
hidden, which makes sentences less transparent [18]. Black box models have other problems. When they<br>
make wrong predictions, it's hard to understand why they made them in the first place and how to<br>
patch them [17]. Also, black box models are often not that useful? Why funnel public funds into a<br>
crime prediction company when you could invest that money into crime prevention?</p>
<p>A reason some people invoke when pushing for AI in decision-making is that humans are far from<br>
making rational decisions, we are biased. There is evidence that stereotypes affect how judges<br>
evaluate the work of their peers [20]. Some people argue in favor of using COMPAS precisely to<br>
mitigate human biases [21], but as we've seen, that entails unacceptable consequences.<br>
Interpretability is not required in many cases, like if you want to generate ideas, but in any<br>
critical application, black box models are irresponsible because they can perpetuate bad practices.<br>
Open-source, transparent models that can be understood do not have this flaw. We can publicly<br>
inspect and criticize them. <em>Those</em> are the models that should be used in critical applications. No<br>
matter how smart AGI eventually gets, if ever, we would continue to need interpretability to ensure<br>
that it's safe.</p>
<h2>III. Responsibility</h2>
<p>A related problem is responsibility: if AI makes an error or a contribution, who takes the blame or<br>
the credit? In pretty much all justice systems, a person has to be held responsible. Air Canada, an<br>
airline of questionable quality, claimed that it could not "be held liable for the information<br>
provided by" a chatbot that had hallucinated a compensation to a customer. A British Columbian<br>
tribunal found Air Canada liable though, it's never legal to lie to your customers, even when you do<br>
it through a chatbot [22]. Something similar happens with copyright: only humans can file patent<br>
demands because AI cannot be credited for inventions [23]. AI-generated images are not protected<br>
by copyright [24]. And I'm not worried about that changing. I don't think any regulator is<br>
interested in transferring accountability or credit or personhood to AI because, then, anyone could<br>
start denying their own responsibilities or claiming someone else's copyrights, it would be chaos.</p>
<p>But, I am worried about some decision makers delegating complex choices to computers. For instance,<br>
in 2020, Stanford hospital executives decided to give COVID vaccines in priority to doctors working<br>
at home instead of resident doctors working in the hospital who were much more likely to contract<br>
the virus [25]. This happened because that decision was taken by an algorithm programmed to<br>
prioritize older employees who, as we know, are more likely to develop complications from COVID. And<br>
work at home, in that specific case. So the executives were well-intentioned: they calibrated the<br>
model to protect more vulnerable employees, but when it proposed an unethical decision, the<br>
executives accepted it thoughtlessly instead of evaluating it critically. Although legal systems<br>
will probably continue to hold AI unaccountable, many people will try to hide behind AI to avoid<br>
making choices themselves. Like, imagine how you'd feel if you realized that your government let<br>
ChatGPT make decisions about healthcare [27]. Not good! We elect people to represent us as well as<br>
they can, not mechanically execute decisions proposed by AI. There should be more transparency in<br>
how decisions are taken, and AI use should always be disclosed.</p>
<h2>IV. Anthropomorphization</h2>
<p>Another reason why people exaggerate the risks of AI singularity so much is anthropomorphization,<br>
that is, assigning human traits to non-human things. In 2022, Blake Lemoine, an engineer at Google,<br>
was fired after publicly saying that an AI model on which he was working was sentient [28]. Like, it<br>
had feelings and all. This is an impression, language models approximate text, they are not alive.<br>
But mammalian brains evolved to recognize patterns, so it's kind of expected that some people would<br>
fall for the illusion that AI is sentient. I mean, people felt bad about forgetting their Tamagotchi<br>
[29], it's not entirely new but it's getting more intense with LLMs.</p>
<p>You can find people online saying that ChatGPT is their friend [30]. Others create AI companions to<br>
simulate interactions with deceased loved ones [31]. These people are allowed to use AI and grieve<br>
the way they want - I want to make that clear. However they have to keep in mind that those are not<br>
real interactions. AI companions are not friends. They are products made by companies that don't<br>
care about their customers and will deny all responsibility when they are involved in tragic<br>
deaths [32]. Some people seem to enjoy the idea of customizing a companion that's perfect for them<br>
[33] instead of meeting real people, which I don't really get. Human connections are meaningful<br>
precisely because they cannot be controlled and adapted to our every likes and needs. Modern life is<br>
already atomized, I get why some people are drawn to AI companions, but rather than forfeiting our<br>
interactions to tech companies, we should promote real friendship and community.</p>
<p>I dislike AI companions, but at least the people who use them understand that they are talking to a<br>
machine. In most cases. Sometimes, anthropomorphization can reach, hum, dangerous levels. To be<br>
clear: people with mental disorders should never discriminated and we have a duty to support them.<br>
Now, I don't want to generalize, my intention is not to insult anyone, but <em>some</em> of the people who<br>
believe that chatbots are sentient have mental disorders [34]. There are people who develop<br>
delusions with language models and, paired with conditions like psychosis, that can cause real<br>
distress. I'm not shaming them here, I'm saying that chatbots can amplify their symptoms, and<br>
companies don't seem to care. And <em>that</em> worries me, not the idea that AI develops sentiments, which<br>
I don't think will happen anyways. AI can also have the opposite effect. Limbic, for example, is a<br>
chatbot that <em>encourages</em> people to consult mental health professionals [35]. It's not all bad, with<br>
proper safety measures, AI can promote mental health, but its harm has to be documented and<br>
regulated.</p>
<h2>V. Knowledge</h2>
<p>A great promise of AI is its ability make knowledge more accessible. I'm skeptical of that. A 2025<br>
study found that language models erode mental abilities [36]. This is not an opinion I have, people<br>
who offload work to AI become worse at critical thinking. Maybe that's a reason why so many people<br>
believe that AGI is so close. It's because they become so dependent on AI that they view it as more<br>
intelligent than it is, not realizing that <em>they</em> got worse in the process. I've heard people say<br>
that writing as a skill is going to disappear because the only thing that matters now is our ability<br>
to judge the quality of LLM outputs. But evaluating a text is a skill developed by putting your own<br>
ideas into words and being critical of the things you read. If we start accepting everything<br>
chatbots say, we'll just become less skilled and more insipid. AI is also linked to a growth in<br>
low-quality scientific publications [37], so even though some AI models do contribute to science if<br>
we use them properly, others can spill disinformation. This growing wave of AI slop science scares<br>
me more than the singularity ever could. So I'm not saying you have to stop using AI, I'm saying:<br>
learn its negative side effects to make more informed decisions about your usage.</p>
<p>Another potential problem with AI is its capacity to indoctrinate. Language models have been shown<br>
to be more persuasive than humans in text-based interactions [38]. There's already a worldwide<br>
problem with disinformation and, if we throw in persuasive chatbots and deepfakes, that could get<br>
even worse. That can also fail spectacularly, the Twitter chatbot is a good example of that, but I<br>
can definitely imagine things taking a darker turn if the AI is controlled by a non-intoxicated<br>
person. To be honest, I don't really know how we should handle the harms of AI on mental abilities<br>
and disinformation. I guess school curricula will have to get updated to improve critical thinking.<br>
Hum. Next topic.</p>
<h2>VI. Resources</h2>
<p>The amount of resources that AI uses is becoming more and more problematic. Training AI models, as<br>
we know, takes a lot of data, and to amass as much of them as possible, tech companies have shown<br>
little respect for copyright. They use pirated books and overlook the terms of use of websites to<br>
extract data. And there's always the argument that "actually, AI does not violate copyright, it<br>
transforms data into a new form, just like how humans don't copy but take inspiration from other<br>
work, so it's not infringing on any copyright...". It is infringing on copyright. Just ask OpenAI<br>
what they think when someone takes inspiration in their work [39]. People don't want their work<br>
stolen, so the Internet has started to close up. Terms of use have changed, and websites require<br>
payment to access their data. At one point, the volume of fresh data might diminish and make models<br>
harder to train [40].</p>
<p>I could point out the amount of electricity and water that generative AI consumes, which has been<br>
abundantly discussed [41]. But to be honest I don't think that is ever going to motivate any change.<br>
Search engines consume electricity, that has never prevented us from using them. Same for Bitcoins<br>
[42]. Yes, language models are worse for the environment than Internet searches, yes that's<br>
horrible, and, yes, people don't care. Our lifestyle is becoming less sustainable over time because<br>
companies and making useless products and people are buying them [43]. If generative AI makes money,<br>
companies will just build power plants to deliver it. And that trend will continue unless we rethink<br>
this entire economic system.</p>
<p>Also, what's the point of using this much AI? We are flooded with AI-generated images, podcasts,<br>
opinions. Bots are replying to each other's comments. People are using it to write their emails. An<br>
article I really like is "I'd rather read the prompt" [44]. It explains that the output of LLMs is<br>
often less valuable than what you provided to it because chatbots add a lot of filler words. And I<br>
don't like wasting my time reading a text that could have been a single sentence.</p>
<h2>VII. Automation</h2>
<p>Ok new section: will AI automate jobs to the point of causing mass unemployment. It depends on who<br>
you ask, some think that new jobs will keep appearing to replace the automated ones [62], others,<br>
that AI will make everyone obsolete [63]. Depending on your mood, that can lead to an unemployment<br>
hellscape OR a new renaissance where people are free from working. We don't know how things will<br>
turn out. Conjecturing about the future can be fun but it's not really useful, so let's instead talk<br>
about how workers should react to automation.</p>
<p>It's clear that, given our current economic model, large-scale automation would be a nightmare.<br>
The companies making the chatbots or robots or whatever would hoard all the money and inequalities<br>
would get even worse. Our laws are not thought out to manage that kind of scenario, and, honestly,<br>
if AGI were to happen tomorrow, I fully believe that governments would remain completely static and<br>
just watch as a few people syphon all the money to the top as they've been doing for decades [45].<br>
Again, this is not a new problem and we have to address it. We have to organize into stronger<br>
unions, use more open-source software, vote laws that force AI companies to repay the money they<br>
make from stolen material, fight tax havens. This does not happen overnight, it requires cooperation<br>
between countries and between social groups, it's so hard to do but it's possible. Instead of<br>
theorizing about whether AGI will eventually replace us we should realize that workers are CURRENTLY<br>
treated unfairly and we need to fight inequalities more.</p>
<p>Inequality is one thing, another problem related to automation that I hear less often talked about<br>
is why we overproduce and overconsume so much in the first place. As I said in the last section, we<br>
are producing more and more stuff in an unsustainable way. I don't have numbers to back this up, but<br>
if we were to decrease our consumption levels, reducing the average workweek to like two days a week<br>
is probably conceivable. A lot of jobs are not very useful, they are just meant to keep the system<br>
running or make useless products. If society were organized to be more efficient, we could all work<br>
less. But we are not headed in this direction, we always want more. The fantasy of AI making us all<br>
live like billionaires will never happen because, once you achieve it and reach a new consumption<br>
baseline, people will keep wanting even more stuff than their neighbor instead of enjoying their<br>
free time [47]. That cannot go on forever because our planet has finite resources, so sooner or<br>
later we'll have to learn to be satisfied with what we have.</p>
<h2>VIII. Takeover</h2>
<p>The last thing I want to address is the AI takeover. The Skynet scenario - I've never watched the<br>
movie - in which AI becomes evil. History is full of doomsayers who predicted the end of the world,<br>
but this time, it's real guys.</p>
<p>There are some reasons why a takeover <em>could</em> happen. A malicious government could develop a system<br>
that gets a little too good at maintaining public order. An AI model trained to replicate human<br>
behaviors could try to self-preserve like a human would [48].But the idea that AI completely goes<br>
loose and no one can stop it is a fantasy. Like don't give it access to all the weapons in the first<br>
place? And if the chabot becomes evil, just unplug it? Jumping from a boxed AI model to an<br>
omnipresent threat requires wild leaps in logic. Whenever you read AI takeover scenarios [49], it<br>
starts with level-headed assertions, but you always get to a point where the authors go full<br>
fanfiction mode: out of nowhere, the AI circumvents all security mechanisms, spreads all over the<br>
Internet, convinces humans to plug it into weapon systems, builds factories to manufacture human<br>
killing drones, and takes over everything. But how? Missile launchers are air-gapped, you cannot<br>
hack into them to make a firework. You cannot setup a secret weapon manufacturing plant without the<br>
government noticing. "But AI will get supersmart! It will trick everyone! It will create physically<br>
impossible things! If you can imagine something, AI will necessarily be able to do it!". This is not<br>
logical argumentation. This is magical thinking. Wanna-be sci-fi writers think they are seeing the<br>
future but they just badly parrot movie scripts.</p>
<p>Honestly, I think that these speculations tend to be unproductive and unhealthy. Don't get me wrong,<br>
it's totally fine to be entertained by the idea of an AI takeover - the Matrix is one of my favorite<br>
movies. I aso think it's good to plan in advance a legal framework to limit dangerous AI usage, and<br>
we have to monitor AI companies, well, ALL companies, to ensure they don't do sketchy things. But,<br>
many people who spend too much time thinking about an AI takeover end up experiencing distress over<br>
a scenario that remains hypothetical [50]. It's like catastrophizing about nuclear war or<br>
antimicrobial-resistant bacteria. It's good to be aware of large-scale problems and act upon them if<br>
you can but there's no point in experiencing Angst in your daily life over them. Fearmongerers don't<br>
realize it, but when they write sensationalistic pieces without contextualizing them properly, they<br>
make vulnerable people panic for real. If you happen to find yourself stuck in a very negative<br>
mindset about the future, it might be time to step back, go outside, and maybe talk to mental health<br>
professionals. This might be <em>the</em> hot take of the video: your own mental health matters more than<br>
the singularity ever will.</p>
<h2>Endsay</h2>
<p>I guess I'll receive comments that go: "you're just in denial. AGI is progressing fast and will<br>
become smarter than us, interpretable, accountable, and will replace us no matter what". Oh really?<br>
Well, maybe progress in AI will stall. Maybe AI will come to be recognized as unsafe and left unused<br>
in many applications. Vielleicht a solar flare will fry up all the GPUs on the planet. See, I can<br>
also write fanfics instead of using real evidence. Our predictions of the future are often<br>
exaggerated and inaccurate, and even though there are many reasons to be enthusiastic about AI, I<br>
don't buy into all of its promises. Besides, even if AGI ever happens, all the points I've presented<br>
would remain pertinent. The purpose of this video is not only to temperate the exaggerations around<br>
AI, it is mainly to bring more focus on problems that should really be getting more attention.</p>
<p>I'd compare our attitude toward the singularity with climate change. Some people like to speculate,<br>
write books, and make movies about an eventual climate catastrophe. Some real projects, like that<br>
Norwegian seed vault [51], are even designed with a doomsday scenario in mind. And sure, that<br>
doesn't hurt if you have time and cash to burn, but climate change is not some cool faraway problem<br>
to daydream about. It's not an exciting movie scenario. We are in it, and it's bleak. It's watching<br>
forests burn down in indifference, electing politicians who promise to gut sustainability programs.<br>
Talking about the end of the world instead of focusing on real problems has not motivated change, it<br>
just made us bored and unresponsive.</p>
<p>And we see the same thing happening with AI. Companies with hypothetical technologies and weird<br>
projects [52] distract us from real problems and attempt to disempower workers. That makes us less<br>
effective at organizing and pushing for better regulations that we need <em>now</em>. I'm not scared of AI,<br>
but I am definitely scarred of what powerful people will do with it. I mean, look at what they<br>
did to the Internet. It was supposed to connect everyone, to make dictatorships impossible, to make<br>
education free. And it did fulfill its promises in some regards - it IS easier to connect and learn<br>
than it was before, but large companies have also gifted us with echo chambers and mental health<br>
problems. If we want AI to benefit everyone, we have to use open-source software as much as<br>
possible. Vote for people who care about our rights. Denounce companies that don't respect them.<br>
Criticize overconsumption. Organize with other workers to negotiate more effectively. Responsible AI<br>
usage won't happen overnight, but these actions will make technology safer, promote sustainability,<br>
and give more power to the people.</p>
<h2 id="references">References</h2>
<p>Hinweis: The order is a little messed up because I added new sources and removed the ones I ended<br>
up not using during editing.</p>
<ul>
<li>[1] Wikipedia. "List of predictions for autonomous Tesla vehicles by Elon Musk". <a href="https://en.wikipedia.org/wiki/List_of_predictions_for_autonomous_Tesla_vehicles_by_Elon_Musk" rel="nofollow">https://en.wikipedia.org/wiki/List_of_predictions_for_autonomous_Tesla_vehicles_by_Elon_Musk</a>
<ul>
<li>The fact alluded in the video is described in the third row of the table, which refers to:<br>
Voelcker, John (October 16, 2015). "Tesla Autopilot: The 10 Most Important Things You Need To Know". Green Car Reports. Retrieved 1 August 2023.</li>
</ul>
</li>
<li>[2] Connor Jewiss. "Why Amazon Fresh’s ‘just walk out’ tech is the shopping experience we all need" (May 17, 2022). Stuff. <a href="https://www.stuff.tv/features/amazon-fresh-just-walk-out-is-the-shopping-experience-we-need/" rel="nofollow">https://www.stuff.tv/features/amazon-fresh-just-walk-out-is-the-shopping-experience-we-need/</a></li>
<li>[3] Alex Bitter. "Amazon's Just Walk Out technology relies on hundreds of workers in India watching you shop" (April 3, 2024). Business Insider. <a href="https://www.businessinsider.com/amazons-just-walk-out-actually-1-000-people-in-india-2024-4?op=1" rel="nofollow">https://www.businessinsider.com/amazons-just-walk-out-actually-1-000-people-in-india-2024-4?op=1</a></li>
<li>[4] Scott Wu. "Introducing Devin, the first AI software engineer" (March 2, 2024). Cognition AI. <a href="https://cognition.ai/blog/introducing-devin" rel="nofollow">https://cognition.ai/blog/introducing-devin</a></li>
<li>[5] "AI Won’t Solve Your Developer Productivity Problems for You" (October 18, 2024). Uplevel. <a href="https://uplevelteam.com/blog/ai-for-developer-productivity" rel="nofollow">https://uplevelteam.com/blog/ai-for-developer-productivity</a></li>
<li>[6] Ben Goertzel. "Artificial General Intelligence: Concept, State of the Art, and Future Prospects" (Dec 29, 2014). Journal of Artificial General Intelligence. <a href="https://sciendo.com/article/10.2478/jagi-2014-0001" rel="nofollow">https://sciendo.com/article/10.2478/jagi-2014-0001</a></li>
<li>[7] perplexity.ai. "Altman Predicts AGI by 2025" (Nov 11, 2024). <a href="https://www.perplexity.ai/page/altman-predicts-agi-by-2025-tUwvEDkiQ9.auqNAMT0X5A" rel="nofollow">https://www.perplexity.ai/page/altman-predicts-agi-by-2025-tUwvEDkiQ9.auqNAMT0X5A</a>
<ul>
<li>Also in support of this point: Darren Orf. "Humanity May Achieve the Singularity Within the Next 6 Months, Scientists Suggest" (Jun 01, 2025). Popular Mechanics. <a href="https://www.popularmechanics.com/science/a64929206/singularity-six-months/" rel="nofollow">https://www.popularmechanics.com/science/a64929206/singularity-six-months/</a></li>
</ul>
</li>
<li>[8] Vernor Vinge. "The coming technological singularity: How to survive in the post-human era" (December 1, 1993). NASA. Lewis Research Center, Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace. <a href="https://ntrs.nasa.gov/citations/19940022856" rel="nofollow">https://ntrs.nasa.gov/citations/19940022856</a></li>
<li>[9] Subbarao Kambhampati. "Can large language models reason and plan?" (6 March 2024). Annals of the New York Academy of Sciences. <a href="https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.15125" rel="nofollow">https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.15125</a>
<ul>
<li>Preprint available for free at <a href="https://arxiv.org/abs/2403.04121" rel="nofollow">https://arxiv.org/abs/2403.04121</a></li>
</ul>
</li>
<li>[10] Jason Wei et al. "Emergent Abilities of Large Language Models" (15 Jun 2022). ArXiv. <a href="https://arxiv.org/pdf/2206.07682" rel="nofollow">https://arxiv.org/pdf/2206.07682</a></li>
<li>[11] Rylan Schaeffer, Brando Miranda, Sanmi Koyejo. "Are Emergent Abilities of Large Language Models a Mirage?" (2023).  Advances in Neural Information Processing Systems 36 (NeurIPS 2023). <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html" rel="nofollow">https://proceedings.neurips.cc/paper_files/paper/2023/hash/adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html</a></li>
<li>[12] Ziwei Xu, Sanjay Jain, Mohan Kankanhalli. "Hallucination is Inevitable: An Innate Limitation of Large Language Models" (2024). ArXiv. <a href="https://arxiv.org/abs/2401.11817" rel="nofollow">https://arxiv.org/abs/2401.11817</a></li>
<li>[13] Iman Mirzadeh at al. "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models" (2024). <a href="https://arxiv.org/pdf/2410.05229" rel="nofollow">https://arxiv.org/pdf/2410.05229</a></li>
<li>[14] Tao Tu et al. "Towards conversational diagnostic artificial intelligence" (2025). Nature. <a href="https://www.nature.com/articles/s41586-025-08866-7" rel="nofollow">https://www.nature.com/articles/s41586-025-08866-7</a></li>
<li>[15] Zhang v. Chen, 2024 BCSC 285. <a href="https://www.bccourts.ca/jdb-txt/sc/24/02/2024BCSC0285cor1.htm" rel="nofollow">https://www.bccourts.ca/jdb-txt/sc/24/02/2024BCSC0285cor1.htm</a></li>
<li>[16] ASTUTE Analytica. "Global Translation Service Market" (2025). <a href="https://www.astuteanalytica.com/industry-report/translation-service-market" rel="nofollow">https://www.astuteanalytica.com/industry-report/translation-service-market</a></li>
<li>[17] Cynthia Rudin. "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead" (2019). Nature Machine Intelligence. <a href="https://www.nature.com/articles/s42256-019-0048-x" rel="nofollow">https://www.nature.com/articles/s42256-019-0048-x</a>
<ul>
<li>Preprint available for free at <a href="https://arxiv.org/pdf/1811.10154" rel="nofollow">https://arxiv.org/pdf/1811.10154</a></li>
</ul>
</li>
<li>[18] Iñigo De Miguel Beriain. "Does the use of risk assessments in sentences respect the right to due process? A critical analysis of the Wisconsin v. Loomis ruling" (2018).  Law, Probability and Risk. <a href="https://academic.oup.com/lpr/article/17/1/45/4877957" rel="nofollow">https://academic.oup.com/lpr/article/17/1/45/4877957</a></li>
<li>[20] John Szmer et al. "Who Shapes the Law? Gender and Racial Bias in Judicial Citations" (2023). APSR. <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/who-shapes-the-law-gender-and-racial-bias-in-judicial-citations/497727B157CB9C6C4C6E03A02B92145C#" rel="nofollow">https://www.cambridge.org/core/journals/american-political-science-review/article/who-shapes-the-law-gender-and-racial-bias-in-judicial-citations/497727B157CB9C6C4C6E03A02B92145C#</a></li>
<li>[21] Konstantin Chatziathanasiou. "Beware the Lure of Narratives: 'Hungry Judges' Should Not Motivate the Use of 'Artificial Intelligence' in Law". (2022). German Law Journal. <a href="https://www.cambridge.org/core/journals/german-law-journal/article/beware-the-lure-of-narratives-hungry-judges-should-not-motivate-the-use-of-artificial-intelligence-in-law/734C6F05568636FE09A26D1C4D52D627" rel="nofollow">https://www.cambridge.org/core/journals/german-law-journal/article/beware-the-lure-of-narratives-hungry-judges-should-not-motivate-the-use-of-artificial-intelligence-in-law/734C6F05568636FE09A26D1C4D52D627</a></li>
<li>[22] Moffatt v. Air Canada, 2024 BCCRT 149 <a href="https://decisions.civilresolutionbc.ca/crt/crtd/en/item/525448/index.do" rel="nofollow">https://decisions.civilresolutionbc.ca/crt/crtd/en/item/525448/index.do</a></li>
<li>[23] United States Patent and Trademark Office, Department of Commerce. "Inventorship Guidance for AI-Assisted Inventions" (2024). <a href="https://www.federalregister.gov/documents/2024/02/13/2024-02623/inventorship-guidance-for-ai-assisted-inventions" rel="nofollow">https://www.federalregister.gov/documents/2024/02/13/2024-02623/inventorship-guidance-for-ai-assisted-inventions</a></li>
<li>[24] THALER v. PERLMUTTER et al, No. 1:2022cv01564 - Document 24 (D.D.C. 2023). <a href="https://law.justia.com/cases/federal/district-courts/district-of-columbia/dcdce/1:2022cv01564/243956/24/" rel="nofollow">https://law.justia.com/cases/federal/district-courts/district-of-columbia/dcdce/1:2022cv01564/243956/24/</a></li>
<li>[25] Alyse Stanley. "A Faulty Algorithm Screwed Residents Out of Stanford’s Vaccine Distribution Plan" (2020). Gizmodo. <a href="https://gizmodo.com/a-faulty-algorithm-screwed-residents-out-of-stanfords-v-1845917350" rel="nofollow">https://gizmodo.com/a-faulty-algorithm-screwed-residents-out-of-stanfords-v-1845917350</a></li>
<li>[26] Simon Willison's Weblog. "A computer can never be held accountable". <a href="https://simonwillison.net/2025/Feb/3/a-computer-can-never-be-held-accountable/" rel="nofollow">https://simonwillison.net/2025/Feb/3/a-computer-can-never-be-held-accountable/</a>
<ul>
<li>This reference is not used in the final version.</li>
</ul>
</li>
<li>[27] Dani Blum and Maggie Astor. "White House Health Report Included Fake Citations" (May 29, 2025). The New York Times. <a href="https://www.nytimes.com/2025/05/29/well/maha-report-citations.html" rel="nofollow">https://www.nytimes.com/2025/05/29/well/maha-report-citations.html</a></li>
<li>[28] Tiffany Wertheimer. "Blake Lemoine: Google fires engineer who said AI tech has feelings" (22 July 2022). BBC. <a href="https://www.bbc.com/news/technology-62275326" rel="nofollow">https://www.bbc.com/news/technology-62275326</a></li>
<li>[29] Wikipedia. "Tamagotchi effect" (2025). <a href="https://en.wikipedia.org/wiki/Tamagotchi_effect" rel="nofollow">https://en.wikipedia.org/wiki/Tamagotchi_effect</a></li>
<li>[30] Derek Thompson. "The antisocial century, in three parts" (8:26 AM - May 26, 2025). Twitter (X). <a href="https://x.com/DKThomp/status/1926978180054724748" rel="nofollow">https://x.com/DKThomp/status/1926978180054724748</a></li>
<li>[31] Mouhamad Rachini. "Some Canadians are using AI simulations to reconnect with their deceased loved ones" (Sep 21, 2023 11:10 AM EDT). CBC Radio. <a href="https://www.cbc.ca/radio/thecurrent/some-canadians-are-using-ai-simulations-to-reconnect-with-their-deceased-loved-ones-1.6949550" rel="nofollow">https://www.cbc.ca/radio/thecurrent/some-canadians-are-using-ai-simulations-to-reconnect-with-their-deceased-loved-ones-1.6949550</a></li>
<li>[32] Kate Payne. "An AI chatbot pushed a teen to kill himself, a lawsuit against its creator alleges" (6:32 PM UTC−4, October 25, 2024). Associated Press. <a href="https://apnews.com/article/chatbot-ai-lawsuit-suicide-teen-artificial-intelligence-9d48adc572100822fdbc3c90d1456bd0" rel="nofollow">https://apnews.com/article/chatbot-ai-lawsuit-suicide-teen-artificial-intelligence-9d48adc572100822fdbc3c90d1456bd0</a></li>
<li>[33] "Paris Hilton's My New BFF". <a href="https://www.sidereel.com/tv-shows/paris-hiltons-my-new-bff" rel="nofollow">https://www.sidereel.com/tv-shows/paris-hiltons-my-new-bff</a></li>
<li>[34] Miles Klee. "People Are Losing Loved Ones to AI-Fueled Spiritual Fantasies" (May 4, 2025). Rolling Stone. <a href="https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/" rel="nofollow">https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/</a></li>
<li>[35] Rhiannon Williams. "A chatbot helped more people access mental-health services" (February 5, 2024). MIT Technology Review. <a href="https://www.technologyreview.com/2024/02/05/1087690/a-chatbot-helped-more-people-access-mental-health-services/" rel="nofollow">https://www.technologyreview.com/2024/02/05/1087690/a-chatbot-helped-more-people-access-mental-health-services/</a></li>
<li>[36] Michael Gerlich. "AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical Thinking" (3 January 2025). Societies. <a href="https://www.mdpi.com/2075-4698/15/1/6" rel="nofollow">https://www.mdpi.com/2075-4698/15/1/6</a></li>
<li>[37] Miryam Naddaf. "AI linked to explosion of low-quality biomedical research papers" (21 May 2025). Nature. <a href="https://www.nature.com/articles/d41586-025-01592-0" rel="nofollow">https://www.nature.com/articles/d41586-025-01592-0</a></li>
<li>[38] Francesco Salvi et al. "On the conversational persuasiveness of GPT-4" (19 May 2025). Nature Human Behavior. <a href="https://www.nature.com/articles/s41562-025-02194-6" rel="nofollow">https://www.nature.com/articles/s41562-025-02194-6</a></li>
<li>[39] Sean Endicott. "OpenAI and Microsoft ironically accuse DeepSeek of copyright infringement — training its cost-effective model with privileged data" (January 29, 2025). Windows Central. <a href="https://www.windowscentral.com/software-apps/openai-accuses-deepseek-of-using-data-without-permission-fails-to-see-irony" rel="nofollow">https://www.windowscentral.com/software-apps/openai-accuses-deepseek-of-using-data-without-permission-fails-to-see-irony</a></li>
<li>[40] Kevin Roose. "The Data That Powers A.I. Is Disappearing Fast" (July 19, 2024). The New York Times. <a href="https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html" rel="nofollow">https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html</a></li>
<li>[41] World Economic Forum. "AI's energy dilemma: Challenges, opportunities, and a path forward" (Jan 21, 2025). <a href="https://www.weforum.org/stories/2025/01/ai-energy-dilemma-challenges-opportunities-and-path-forward/" rel="nofollow">https://www.weforum.org/stories/2025/01/ai-energy-dilemma-challenges-opportunities-and-path-forward/</a></li>
<li>[42] Cristina Criddle. "Bitcoin consumes 'more electricity than Argentina'" (10 February 2021). BBC. <a href="https://www.bbc.com/news/technology-56012952" rel="nofollow">https://www.bbc.com/news/technology-56012952</a></li>
<li>[43] Earth Overshoot Day. "Past Earth Overshoot Days" (Consulted June 13, 2025). <a href="https://www.overshootday.org/newsroom/past-earth-overshoot-days/" rel="nofollow">https://www.overshootday.org/newsroom/past-earth-overshoot-days/</a></li>
<li>[44] Clayton Ramsey . "I'd rather read the prompt" (2025-05-03). <a href="https://claytonwramsey.com/blog/prompt/" rel="nofollow">https://claytonwramsey.com/blog/prompt/</a></li>
<li>[45] Pew Research Center. "Economic Inequality" (January 9, 2020). <a href="https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/" rel="nofollow">https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/</a></li>
<li>[47] LearnVest. "Would You Rather Have More Time Or More Money?" (Dec 20, 2013). Forbes. <a href="https://www.forbes.com/sites/learnvest/2013/12/20/would-you-rather-have-more-time-or-more-money/" rel="nofollow">https://www.forbes.com/sites/learnvest/2013/12/20/would-you-rather-have-more-time-or-more-money/</a></li>
<li>[48] Meinke et al. "Frontier Models are Capable of In-context Scheming" (2024-12-05). <a href="https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/6751eb240ed3821a0161b45b/1733421863119/in_context_scheming_reasoning_paper.pdf" rel="nofollow">https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/6751eb240ed3821a0161b45b/1733421863119/in_context_scheming_reasoning_paper.pdf</a></li>
<li>[49] Daniel Kokotajlo at al. "AI 2027" (April 2025). <a href="https://ai-2027.com/" rel="nofollow">https://ai-2027.com/</a>
<ul>
<li>Don't waste your time reading that, there are actually good sci-fi books out there.</li>
</ul>
</li>
<li>[50] Alkhalifah1 et al. "Existential anxiety about artificial intelligence (AI)- is it the end of humanity era or a new chapter in the human revolution: questionnaire-based observational study" (08 April 2024). Frontiers in Psychiatry. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11036542/" rel="nofollow">https://pmc.ncbi.nlm.nih.gov/articles/PMC11036542/</a></li>
<li>[51] <a href="https://www.seedvault.no/" rel="nofollow">https://www.seedvault.no/</a></li>
<li>[52] <a href="https://www.technologyreview.com/2025/05/05/1116090/bryan-johnson-new-religion-body-is-god/" rel="nofollow">https://www.technologyreview.com/2025/05/05/1116090/bryan-johnson-new-religion-body-is-god/</a></li>
<li>[53] Equivant. "Debunking Misconceptions About the COMPAS Core Instrument: What You Need to Know" (2024). <a href="https://equivant-supervision.com/debunking-misconceptions-about-the-compas-core-instrument-what-you-need-to-know/" rel="nofollow">https://equivant-supervision.com/debunking-misconceptions-about-the-compas-core-instrument-what-you-need-to-know/</a></li>
<li>[54] Conor Murray. "Why AI ‘Hallucinations’ Are Worse Than Ever" (6 May 2025). Forbes. <a href="https://www.forbes.com/sites/conormurray/2025/05/06/why-ai-hallucinations-are-worse-than-ever/" rel="nofollow">https://www.forbes.com/sites/conormurray/2025/05/06/why-ai-hallucinations-are-worse-than-ever/</a></li>
<li>[55] Jessica Hamzelou. "Bryan Johnson wants to start a new religion in which 'the body is God'" (May 5, 2025). MIT Technology Review. <a href="https://machinelearning.apple.com/research/illusion-of-thinking" rel="nofollow">https://machinelearning.apple.com/research/illusion-of-thinking</a></li>
<li>[56] Gordon, Cindy. "ChatGPT And Generative AI Innovations Are Creating Sustainability Havoc". Forbes. <a href="https://www.forbes.com/sites/cindygordon/2024/03/12/chatgpt-and-generative-ai-innovations-are-creating-sustainability-havoc/" rel="nofollow">https://www.forbes.com/sites/cindygordon/2024/03/12/chatgpt-and-generative-ai-innovations-are-creating-sustainability-havoc/</a></li>
<li>[57] Engel et al. "Code is law: how COMPAS affects the way the judiciary handles the risk of recidivism" (2024). Artificial Intelligence and Law. <a href="https://link.springer.com/article/10.1007/s10506-024-09389-8" rel="nofollow">https://link.springer.com/article/10.1007/s10506-024-09389-8</a></li>
<li>[58] Hacker News forum thread "Hallucination is inevitable: An innate limitation of large language models". <a href="https://news.ycombinator.com/item?id=39499207" rel="nofollow">https://news.ycombinator.com/item?id=39499207</a></li>
<li>[59] Christopher Reynolds. "Air Canada ordered to pay passengers $10M in damages after class action over ticket prices" (23 April 2025). CBC. <a href="https://www.cbc.ca/news/canada/montreal/air-canada-class-action-ruling-1.7516556" rel="nofollow">https://www.cbc.ca/news/canada/montreal/air-canada-class-action-ruling-1.7516556</a></li>
<li>[60] Twitter post: <a href="https://x.com/MattBinder/status/1922713839566561313" rel="nofollow">https://x.com/MattBinder/status/1922713839566561313</a></li>
<li>[61] Alex Reisner. "The Unbelievable Scale of AI’s Pirated-Books Problem" (March 20, 2025). The Atlantic. <a href="https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/" rel="nofollow">https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/</a></li>
<li>[62] Julian Horsey. "Sam Altman On How AI Will Reshape Jobs – Are You Ready?" (December 15, 2024). Geeky Gadget. <a href="https://www.geeky-gadgets.com/ai-impact-on-jobs-and-workforce/" rel="nofollow">https://www.geeky-gadgets.com/ai-impact-on-jobs-and-workforce/</a></li>
<li>[63] Jim VandeHei and Mike Allen. "Behind the Curtain: A white-collar bloodbath" (May 28, 2025). Axios. <a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic" rel="nofollow">https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic</a></li>
<li>[64] Simon Walo. "‘Bullshit’ After All? Why People Consider Their Jobs Socially Useless" (July 21, 2023). Work, Employment and Society. <a href="https://journals.sagepub.com/doi/10.1177/09500170231175771" rel="nofollow">https://journals.sagepub.com/doi/10.1177/09500170231175771</a></li>
<li>[65] Wikipedia. "Facebook–Cambridge Analytica data scandal". <a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal" rel="nofollow">https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal</a></li>
<li>[66] J. Alas. "Facebook Accused of Exploiting Teenage Girl Insecurities — Shows Beauty Ads After They Delete Selfies" (03 June 2025, 7:37 AM BST). International Business Times UK. <a href="https://www.ibtimes.co.uk/facebook-accused-exploiting-teenage-girl-insecurities-shows-beauty-ads-after-they-delete-selfies-1734823" rel="nofollow">https://www.ibtimes.co.uk/facebook-accused-exploiting-teenage-girl-insecurities-shows-beauty-ads-after-they-delete-selfies-1734823</a></li>
<li>[67] Instagram page "gay.fabulous" (not sure if it's originally from there though) <a href="https://www.instagram.com/p/DKVX6MRRSC4/" rel="nofollow">https://www.instagram.com/p/DKVX6MRRSC4/</a></li>
<li>[68] Stephanie Ha. "Exclusive: Carney says ‘yes’ to building a pipeline if consensus exists for one" (May 13, 2025 at 8:42PM EDT). CTV News. <a href="https://www.ctvnews.ca/politics/article/exclusive-carney-says-yes-to-building-a-pipeline-if-consensus-exists-for-one/" rel="nofollow">https://www.ctvnews.ca/politics/article/exclusive-carney-says-yes-to-building-a-pipeline-if-consensus-exists-for-one/</a></li>
<li>[69] Wikipedia. "Hockey stick graph". <a href="https://en.wikipedia.org/wiki/Hockey_stick_graph" rel="nofollow">https://en.wikipedia.org/wiki/Hockey_stick_graph</a></li>
</ul></article>
	</body>
</html>
